{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uqbEp31ViBMY",
        "s_1meAhSyd2d",
        "RUaR8buSONmA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95d01231e12f4c84be4f0c3543080a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6f098a73f04f461db52f72e5def92264",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b9deb5e178c744ffaa6af2e10e79c390",
              "IPY_MODEL_d1c43efb5abd48158cdd9739a61a9418",
              "IPY_MODEL_809c9ed7612446b298d8aedad7bb5e66"
            ]
          }
        },
        "6f098a73f04f461db52f72e5def92264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9deb5e178c744ffaa6af2e10e79c390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ffcc54a4974e458faa4e7e4bcc581e61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f418383366b4d3db5f04b62d7353add"
          }
        },
        "d1c43efb5abd48158cdd9739a61a9418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6ec54213372949d786b9bdb1f4e1e73a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 519,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 519,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8be58601b6594ae5b12a77e8a61f1c47"
          }
        },
        "809c9ed7612446b298d8aedad7bb5e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e58c342e8d7a408ea6a89d6dea60f4a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 519/519 [00:06&lt;00:00, 85.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc2db6f2674344ae86cb587ee3083e62"
          }
        },
        "ffcc54a4974e458faa4e7e4bcc581e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f418383366b4d3db5f04b62d7353add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ec54213372949d786b9bdb1f4e1e73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8be58601b6594ae5b12a77e8a61f1c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e58c342e8d7a408ea6a89d6dea60f4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc2db6f2674344ae86cb587ee3083e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "367e8e3186f044b2a6062a980ccf4001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ca38c2126e3409d855117c2a9612072",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46b12b24e3b248bc8ffbdb427e1034cc",
              "IPY_MODEL_11046eb8acc84e9c8fa7cf4a65556a95",
              "IPY_MODEL_0dc1de87d0144fabaf89b2438f240b3d"
            ]
          }
        },
        "0ca38c2126e3409d855117c2a9612072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46b12b24e3b248bc8ffbdb427e1034cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ccd1fb361a874c1d906ed8dbd5ce1f03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0232ad05e7fa43f5af5e4372202432e0"
          }
        },
        "11046eb8acc84e9c8fa7cf4a65556a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba9d2ed703b549c7982a1c5bedf3d03a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 519,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 519,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c20fc9165fab484ea263120afc3634ba"
          }
        },
        "0dc1de87d0144fabaf89b2438f240b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b8b64cc937e4f56a5dca1f03d9f49f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 519/519 [00:03&lt;00:00, 123.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de4413398d854105bb4c119a968660f1"
          }
        },
        "ccd1fb361a874c1d906ed8dbd5ce1f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0232ad05e7fa43f5af5e4372202432e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba9d2ed703b549c7982a1c5bedf3d03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c20fc9165fab484ea263120afc3634ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b8b64cc937e4f56a5dca1f03d9f49f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de4413398d854105bb4c119a968660f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecW467fbNc4N"
      },
      "source": [
        "# YOLOV4 OBJECT DETECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TEYbG1yNTFd"
      },
      "source": [
        "### STEP 1: DARKNET CLONE AND INSTALLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny1G-RZtG4Na"
      },
      "source": [
        "#!unzip -d /content/drive/MyDrive /content/drive/MyDrive/yolov4.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHYXSr5cDHDD"
      },
      "source": [
        "#!zip -r /content/gdrive/MyDrive/yolov4.zip /content/gdrive/MyDrive/yolov4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDaCPqPPMtI5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a49385-a775-4ab4-f53c-581ab48ee20c"
      },
      "source": [
        "# clone darknet repo\n",
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 15308, done.\u001b[K\n",
            "remote: Total 15308 (delta 0), reused 0 (delta 0), pack-reused 15308\u001b[K\n",
            "Receiving objects: 100% (15308/15308), 13.70 MiB | 17.80 MiB/s, done.\n",
            "Resolving deltas: 100% (10400/10400), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxkteNoim6IE"
      },
      "source": [
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/image.c /content/darknet/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZjRJqIjzupb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563cfed2-b7a0-46cc-f556-b7195b171016"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn6ant8qNlYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac473c54-3862-4b94-bda2-cb6163526703"
      },
      "source": [
        "# change makefile to have GPU and OPENCV enabled\n",
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile  #I will use opencv library in darknet\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile #I will use cpu library in darknet\n",
        "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile #cudnn a necessary tool for cpu\n",
        "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHsCoQi7OX13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ac318e-d612-4654-9b06-8b2fdc77e230"
      },
      "source": [
        "# verify CUDA\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UALaKDuxOhGW"
      },
      "source": [
        "# make darknet (builds darknet so that you can then use the darknet executable file to run or train object detectors)\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqbEp31ViBMY"
      },
      "source": [
        "### STEP 2: LOADING THE DATASET WE HAVE PREPARED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Af3q0t-hwnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c667d1c-0ef4-40af-cebe-94fb2f6601eb"
      },
      "source": [
        "# this is where my datasets are stored within my Google Drive (I created a yolov4 folder to store all important files for custom training) \n",
        "#%cd darknet/\n",
        "#!ls /content/drive/MyDrive/yolov4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/yolov4': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M8yiG9Di6yF"
      },
      "source": [
        "# copy over both datasets into the root directory of the Colab VM (comment out test.zip if you are not using a validation dataset)\n",
        "#!cp /content/drive/MyDrive/yolov4/obj.zip ../\n",
        "#!cp /content/drive/MyDrive/yolov4/test.zip ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfaT3dPDi-FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a419b87b-9c8c-4190-f270-9d3522704d5b"
      },
      "source": [
        "# unzip the datasets and their contents so that they are now in /darknet/data/ folder\n",
        "#!unzip ../obj.zip -d data/\n",
        "#!unzip ../test.zip -d data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ../obj.zip\n",
            "checkdir:  cannot create extraction directory: data\n",
            "           File exists\n",
            "Archive:  ../test.zip\n",
            "checkdir:  cannot create extraction directory: data\n",
            "           File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjUwQnxEkY5g"
      },
      "source": [
        "### **STEP 3: LET'S PREPARE THE REQUIRED FILES FOR THE TRAINING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHC8aFRXjGm8"
      },
      "source": [
        "In this step, we will create the .cfg file, obj.data, obj.names and train.txt files required for the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvJu1fzCztYO"
      },
      "source": [
        "download cfg to google drive and change its name                         \n",
        "```\n",
        "from shutil import copy2                                    \n",
        "copy2(\"/content/darknet/cfg/yolov4-custom.cfg\",\"/content/drive/MyDrive/yolov4\")    \n",
        "```      \n",
        "you can also copy like this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOR3wPTUyiSQ"
      },
      "source": [
        "# download cfg to google drive and change its name\n",
        "#%cd darknet/\n",
        "#!cp cfg/yolov4-custom.cfg /content/drive/MyDrive/yolov4/yolov4-obj.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2AJgJeX1d2j"
      },
      "source": [
        "Changes we need to make in our config file:\n",
        "\n",
        "(The values given here are the recommended values for these variables.)\n",
        "\n",
        "1. The batch value in our config file is the number of images we will import into our convolutional neural network in each iteration. Subdivision determines how many subdivisions we will divide each batch into. We can set the subdivision value to 16. **batch = 64 and subdivision 16.**\n",
        "\n",
        "2. Then we can change the image size that will enter the model from the width and height sections. The max_batches value determines how many iterations our model will take. We can set our class number to x2000. Since we will train a model consisting of one class, we set the Max_batches value to 6000. **We equalize max_batches(2000 x number of classes trained). But the minimum we can do is 6000, so if you have one, two and three classes, it should be 6000.**\n",
        "\n",
        "3. Then we change the value of the step to 80% or 90% of our max_batches value. I set the value of the step to 4800 which is 80% of 6000. **We make the values of the steps (80% of max_batches), (90% of max_batches).**\n",
        "\n",
        "4. We replace the classes values under the [yolo] heading with the number of classes we train.\n",
        "\n",
        "5. The Steps parameter is the number of iterations that the learning rate will be reduced to fit our model well. Finally, in our config file, we set the value of the class parameters to 3, which is our class number. We change the Filters parameters to (class + 5)x3. **In our case, this value is 18. We also equalize the filter's variables (number of classes to train + 5 )x3.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK1a9dCIyiuf"
      },
      "source": [
        "# upload the custom .cfg back to cloud VM from Google Drive\n",
        "\n",
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/yolov4-obj.cfg ./cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cInjxJm7GpZ"
      },
      "source": [
        "**-obj.names and obj.data**\n",
        "\n",
        "Let's create a file named obj.names in our folder named yolov3 and write the names of your objects that we will train the file with.\n",
        "\n",
        "E.G:\n",
        "```\n",
        "traffic sign\n",
        "traffic light\n",
        "```\n",
        "\n",
        "In the same folder, we create a file with the name obj.data and write the directory where we will save the number of objects we will train, the addresses of the files named train.txt, text.txt and obj.names that we will use while training, and the weights we find as a result of the training.\n",
        "\n",
        "E.G:\n",
        "```\n",
        "classes = 1\n",
        "train = data/train.txt\n",
        "valid = data/test.txt\n",
        "names = data/obj.names\n",
        "backup = /mydrive/yolov4/backup\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVQZjKcL95vl"
      },
      "source": [
        "upload the obj.names and obj.data files to cloud VM from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6-M5rFK7HN1"
      },
      "source": [
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/obj.names ./data\n",
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/obj.data  ./data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6niAZy24EUSg"
      },
      "source": [
        "#### **-Train and Test Files**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBr35ggE-AZq"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "**generate_train.py**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  import os\n",
        "  image_files = []\n",
        "  os.chdir(os.path.join(\"data\", \"obj\"))\n",
        "  for filename in os.listdir(os.getcwd()):\n",
        "      if filename.endswith(\".jpg\"):\n",
        "          image_files.append(\"data/obj/\" + filename)\n",
        "  os.chdir(\"..\")\n",
        "  with open(\"train.txt\", \"w\") as outfile:\n",
        "      for image in image_files:\n",
        "          outfile.write(image)\n",
        "          outfile.write(\"\\n\")\n",
        "      outfile.close()\n",
        "  os.chdir(\"..\")\n",
        "```\n",
        "\n",
        "**generate_test.py**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  import os\n",
        "\n",
        "  image_files = []\n",
        "  os.chdir(os.path.join(\"data\", \"test\"))\n",
        "  for filename in os.listdir(os.getcwd()):\n",
        "      if filename.endswith(\".jpg\"):\n",
        "          image_files.append(\"data/test/\" + filename)\n",
        "  os.chdir(\"..\")\n",
        "  with open(\"test.txt\", \"w\") as outfile:\n",
        "      for image in image_files:\n",
        "          outfile.write(image)\n",
        "          outfile.write(\"\\n\")\n",
        "      outfile.close()\n",
        "  os.chdir(\"..\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5qJ748-TutK"
      },
      "source": [
        "upload the generate_train.py and generate_test.py script to cloud VM from Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec8OtGiW-Ety"
      },
      "source": [
        "#!cp /content/drive/MyDrive/Studies/Intern/yolov4/generate_train.py ./\n",
        "#!cp /content/drive/MyDrive/Studies/Intern/yolov4/generate_test.py ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMF6d35sUC-4"
      },
      "source": [
        "#!python generate_train.py\n",
        "#!python generate_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrKi1gUQUDeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d65c3b0-ea77-40dd-d710-0e297a663aad"
      },
      "source": [
        "# verify that the newly generated train.txt and test.txt can be seen in our darknet/data folder\n",
        "!ls data/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9k.tree     eagle.jpg\t imagenet.labels.list\t   obj.names\t     voc.names\n",
            "coco9k.map  giraffe.jpg  imagenet.shortnames.list  openimages.names\n",
            "coco.names  goal.txt\t labels\t\t\t   person.jpg\n",
            "dog.jpg     horses.jpg\t obj.data\t\t   scream.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZPKhQgqVFAF"
      },
      "source": [
        "### STEP 4: DOWNLOAD THE WEIGHTS OF PRE-TRAINED CONVOLUTIONAL LAYERS\n",
        "\n",
        "In this step, we download the used deep learning layers weights for the pre-trained yolov3. We do not have to perform this step, but starting the training with these weights will help the model we train to work more accurately and shorten the training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab3ofltAVGOu"
      },
      "source": [
        "#!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSm93uBkVTHV"
      },
      "source": [
        "### STEP 5: TRAINING\n",
        "All the necessary files are ready, we can start the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db6QZ2HVWYFY"
      },
      "source": [
        "Training will begin with the next command.\n",
        "\n",
        "The duration of our training may vary depending on factors such as the number of photos in your data set, the quality of the photos, and the number of objects you train. Our loss value is important for the accuracy of our model. The lower our Loss value, the more accurate our model will work. We can run our model until the loss value stops decreasing and train the most accurate model possible according to our data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVYYLkSmViFg"
      },
      "source": [
        "# train your custom detector! (uncomment %%capture below if you run into memory issues or your Colab is crashing)\n",
        "# %%capture\n",
        "#!./darknet detector train data/obj.data cfg/yolov4-obj.cfg yolov4.conv.137 -dont_show -map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTv7nvWPWoL1"
      },
      "source": [
        "# the graph of our training.\n",
        "#imShow('chart.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIXX6trKW_Gm"
      },
      "source": [
        "If we don't like the weights, we can continue the training from where we left off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe0R7MNJW-lF"
      },
      "source": [
        "#!./darknet detector train data/obj.data cfg/yolov4-obj.cfg /content/drive/MyDrive/Studies/Intern/yolov4/backup/yolov4-obj_last.weights -dont_show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_1meAhSyd2d"
      },
      "source": [
        "### STEP 6: USE OUR TRAINED MODEL\n",
        "\n",
        "\n",
        "Our training is complete, now we can make recognition on the photos we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyJzvhv4X48Z"
      },
      "source": [
        "Let's look at the average loss value and percent accuracy of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630qDfB3ydff"
      },
      "source": [
        "#!./darknet detector map data/obj.data cfg/yolov4-obj.cfg /content/drive/MyDrive/Studies/Intern/yolov4/backup/yolov4-obj_last.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG-MtCljyiwG"
      },
      "source": [
        "#### **Let's run our model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-j-hGJ2x-Le"
      },
      "source": [
        "Let's test our model for a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KatbNMnfRoh0"
      },
      "source": [
        "# define helper functions\n",
        "def imShow(path): \n",
        "  import cv2\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  image = cv2.imread(path)\n",
        "  height, width = image.shape[:2]\n",
        "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(18, 10)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()\n",
        "\n",
        "# use this to upload files\n",
        "def upload():\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload() \n",
        "  for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "      f.write(data)\n",
        "      print ('saved file', name)\n",
        "\n",
        "# use this to download a file  \n",
        "def download(path):\n",
        "  from google.colab import files\n",
        "  files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miiXSHkkxrWT"
      },
      "source": [
        "cp /content/drive/MyDrive/yolov4/test_pred.zip ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Cfz0TbvxrWT"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpv9s-fdxrWU"
      },
      "source": [
        "!unzip /content/test_pred.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkAseV2exrWU"
      },
      "source": [
        "%cd /content/darknet/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lj-UAi4yp_c"
      },
      "source": [
        "# need to set our custom cfg to test mode \n",
        "%cd cfg\n",
        "!sed -i 's/batch=64/batch=1/' yolov4-obj.cfg\n",
        "!sed -i 's/subdivisions=16/subdivisions=1/' yolov4-obj.cfg\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh1CnruZVk4w"
      },
      "source": [
        "To predict and see individual images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfQWsDaGJRHY"
      },
      "source": [
        "To predict and see individual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzATlqJlJRHZ"
      },
      "source": [
        "# run your custom detector with this command (upload an image to your google drive to test, thresh flag sets accuracy that detection must be in order to show it)\n",
        "!./darknet detector test data/obj.data cfg/yolov4-obj.cfg /content/drive/MyDrive/yolov4/backup/yolov4-obj_last.weights  /content/drive/MyDrive/images/cfc_000311.jpg -ext_output -dont_show -out  result.json -thresh 0.3 \n",
        "imShow('predictions.jpg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfR5UDwfTIMN"
      },
      "source": [
        "import json\n",
        "\n",
        "json_file = open('/content/darknet/result.json', 'r')#file reading process\n",
        "json_dict=json.load(json_file)#Contents of json file converted to dict data type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiFltjfBTgUk"
      },
      "source": [
        "json_dict[0][\"objects\"][0][\"relative_coordinates\"]['center_x']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFvNdQNSV3rC"
      },
      "source": [
        "import cv2\n",
        "img= cv2.imread('/content/drive/MyDrive/images/cfc_000311.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk9JZkHrzznK"
      },
      "source": [
        "center_x=img.shape[1]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['center_x']\n",
        "center_y=img.shape[0]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['center_y']\n",
        "width=img.shape[1]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['width']\n",
        "height=img.shape[0]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['height']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byXGrsD-7ExV"
      },
      "source": [
        "x_center=int(xmin + width()/2)\n",
        "\n",
        "\n",
        "y_center=int(ymin + height()/2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlxO3Wqv1MwS"
      },
      "source": [
        "new_width=(img.shape[1]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['width'])/2\n",
        "new_height=(img.shape[0]*json_dict[0][\"objects\"][0][\"relative_coordinates\"]['height'])/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbusekxA4ZkB"
      },
      "source": [
        "x_min=center_x-new_width\n",
        "y_min=center_y-new_height"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb9sMYwy6lSK"
      },
      "source": [
        "width=int(xmax-xmin)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "height=int(ymax-ymin)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FQOQb5F3xWH"
      },
      "source": [
        "x_max=width+x_min\n",
        "y_max=height+y_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiUnAfPjDTPV"
      },
      "source": [
        "w=int(width)\n",
        "h=int(height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ybqFmKP8Pno"
      },
      "source": [
        "crop_img = img[int(y_min):int(y_min)+h,int( x_min):int(x_min)+w]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BUrgcZ2DsRZ"
      },
      "source": [
        "cv2.imwrite('/content/deneme/deneme.jpg',crop_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmOCrrdjysOR"
      },
      "source": [
        "# run your custom detector with this command (upload an image to your google drive to test, thresh flag sets accuracy that detection must be in order to show it)\n",
        "!./darknet detector test data/obj.data cfg/yolov4-obj.cfg /content/drive/MyDrive/yolov4/backup/yolov4-obj_last.weights /content/drive/MyDrive/images/cfc_000311.jpg -thresh 0.3\n",
        "imShow('predictions.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX-UOg0-YBfU"
      },
      "source": [
        "### **STEP 7: PREDICTION WITH TEST DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDACYmMVvjM"
      },
      "source": [
        "**To predict and save multiple images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WAABvvFLh5Q"
      },
      "source": [
        "cp /content/drive/MyDrive/Studies/Intern/yolov4/test_pred.zip ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR7_0BpGeIrl"
      },
      "source": [
        "!unzip /content/test_pred.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts7JfnG2g_Cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6693513e-85c6-4e2f-93dc-46d2fb967539"
      },
      "source": [
        "%cd /content/darknet/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/darknet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9USp054fwIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273d67df-486e-4a1f-e7b6-859c2ecf595d"
      },
      "source": [
        "# need to set our custom cfg to test mode \n",
        "%cd cfg\n",
        "!sed -i 's/batch=64/batch=1/' yolov4-obj.cfg\n",
        "!sed -i 's/subdivisions=16/subdivisions=1/' yolov4-obj.cfg\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet/cfg\n",
            "/content/darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GiFgH6jEs3R"
      },
      "source": [
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/backup/yolov4-obj_last.weights ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deZsUdQMNsm_"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "image_path=\"../test_pred\"\n",
        "image_path_list = glob.glob(os.path.join(image_path, '*'))\n",
        "image_path_list.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmFKa5fSWJkS"
      },
      "source": [
        "def detectionPredict(imageDir):\n",
        "    import glob\n",
        "    import os\n",
        "    import cv2\n",
        "    os.system(\"./darknet detector test data/obj.data cfg/yolov4-obj.cfg ../yolov4-obj_last.weights {} -thresh 0.3\".format(imageDir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glgDDvrOLOU2",
        "outputId": "9d8efe74-4738-4925-e5c9-2db5d4674261"
      },
      "source": [
        "from constant import *\n",
        "import tqdm\n",
        "import torch\n",
        "from preprocessing import tensorize_image, tensorize_mask, image_mask_check\n",
        "import cv2\n",
        "from train import *\n",
        "from PIL import Image\n",
        "\n",
        "for i in tqdm.tqdm(range(len(image_path_list))):\n",
        "    batch_test = image_path_list[i:i+1]\n",
        "    detectionPredict(batch_test[0])\n",
        "    img=cv2.imread(\"predictions.jpg\")\n",
        "    predict_name=batch_test[0]\n",
        "    if not os.path.exists(\"../predict_sign\"):\n",
        "      os.mkdir(\"../predict_sign\")\n",
        "    predict_path=predict_name.replace('test_pred', 'predict_sign')\n",
        "    cv2.imwrite(predict_path,img.astype(np.uint8))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSjhFJeTxTsw"
      },
      "source": [
        "### **STEP 8: Multi Class Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ka-E_kTyivF"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Studies/Intern/For_Colab/* ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTwDJg1YPqJb"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Studies/Intern/models ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM-Eud2HTTZ9"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/data/\"):\n",
        "  os.mkdir(\"/content/data/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwKiRnClKj0J"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Studies/Intern/yolov4/test_pred.zip ../\n",
        "!unzip -d /content/data/ /content/test_pred.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIkjIGdcTJVr",
        "outputId": "1f342134-aa83-466f-d126-1ce93e3ddfec"
      },
      "source": [
        "%cd /content/darknet/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxjTGtqRTJVs"
      },
      "source": [
        "# need to set our custom cfg to test mode \n",
        "%cd cfg\n",
        "!sed -i 's/batch=64/batch=1/' yolov4-obj.cfg\n",
        "!sed -i 's/subdivisions=16/subdivisions=1/' yolov4-obj.cfg\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUYxS5P4TJVt"
      },
      "source": [
        "!cp /content/drive/MyDrive/Studies/Intern/yolov4/backup/yolov4-obj_last.weights ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M33l-AHWK_ZE"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "image_path=\"../test_pred\"\n",
        "image_path_list = glob.glob(os.path.join(image_path, '*'))\n",
        "image_path_list.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNaRHxsTLBMo"
      },
      "source": [
        "def Detection(imageDir):\n",
        "    os.system(\"./darknet detector test data/obj.data cfg/yolov4-obj.cfg ../yolov4-obj_last.weights {} -ext_output -dont_show -out result.json -thresh 0.5\"\n",
        "              .format(imageDir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oohqf64nNuWZ",
        "outputId": "a49ac58b-2d7f-458c-f14c-50a07e5d8c4c"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsoeR5iUQOr5"
      },
      "source": [
        "#### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtSCoC4uLQYN"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import tqdm\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook\n",
        "from preprocess import tensorize_image\n",
        "import numpy as np\n",
        "from constant import *\n",
        "from train import *\n",
        "from natsort import natsorted\n",
        "\n",
        "#### PARAMETERS #####\n",
        "cuda = True\n",
        "test = True\n",
        "predict_name = \"test_pred\"\n",
        "fs_model_name = \"Unet_2.pt\"\n",
        "line_model_name = \"SegNet.pt\"\n",
        "fs_model_path = os.path.join(MODELS_DIR, fs_model_name)\n",
        "line_model_path = os.path.join(MODELS_DIR, line_model_name)\n",
        "input_shape = input_shape\n",
        "#####################\n",
        "\n",
        "if test:\n",
        "    if not os.path.exists(TEST_PREDICT_DIR): \n",
        "      os.mkdir(TEST_PREDICT_DIR)\n",
        "    test_input_path_list = glob.glob(os.path.join(TEST_DIR, \"*\"))\n",
        "    test_input_path_list = natsorted(test_input_path_list)\n",
        "    predict_path = os.path.join(TEST_PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "else:\n",
        "    if not os.path.exists(PREDICT_DIR): \n",
        "      os.mkdir(PREDICT_DIR)\n",
        "    predict_path = os.path.join(PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "\n",
        "if not os.path.exists(predict_path): \n",
        "    os.mkdir(predict_path)\n",
        "\n",
        "# LOAD MODEL\n",
        "fs_model = torch.load(fs_model_path)\n",
        "#Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. \n",
        "#Failing to do this will yield inconsistent inference results.\n",
        "fs_model.eval()\n",
        "\n",
        "line_model = torch.load(line_model_path)\n",
        "line_model.eval()\n",
        "\n",
        "if cuda:\n",
        "    fs_model = fs_model.cuda()\n",
        "    line_model = line_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# PREDICT\n",
        "def predict(fs_model, line_model, images):\n",
        "\n",
        "\n",
        "    for image in tqdm_notebook(images):\n",
        "        img = cv2.imread(image)\n",
        "        batch_test = tensorize_image([image], input_shape, cuda)\n",
        "\n",
        "        Detection(image)\n",
        "        img = cv2.imread(\"predictions.jpg\")\n",
        "        \n",
        "        fs_output = fs_model(batch_test)\n",
        "        line_output = line_model(batch_test)\n",
        "        fs_out = torch.argmax(fs_output, axis=1)\n",
        "        line_out = torch.argmax(line_output, axis=1)\n",
        "\n",
        "        \n",
        "        fs_out_cpu = fs_out.cpu()\n",
        "        line_out_cpu = line_out.cpu()\n",
        "        \n",
        "        fs_outputs_list  = fs_out_cpu.detach().numpy()\n",
        "        line_outputs_list  = line_out_cpu.detach().numpy()\n",
        "        \n",
        "        fs_mask = np.squeeze(fs_outputs_list, axis=0)\n",
        "        line_mask = np.squeeze(line_outputs_list, axis=0)\n",
        "       \n",
        "        fs_mask_uint8 = fs_mask.astype('uint8')\n",
        "        line_mask_uint8 = line_mask.astype('uint8')\n",
        "        \n",
        "        fs_mask_resize = cv2.resize(fs_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_CUBIC)\n",
        "        fs_line_resize = cv2.resize(line_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_NEAREST)\n",
        "        \n",
        "        \n",
        "        #img_resize = cv2.resize(img, input_shape)\n",
        "        mask_ind = fs_mask_resize == 1\n",
        "        mask_ind = fs_line_resize == 1\n",
        "        #copy_img = img_resize.copy()\n",
        "        copy_img = img.copy()\n",
        "        \n",
        "        img[fs_mask_resize==1, :] = (255, 0, 125)\n",
        "        img[fs_line_resize==1, :] = (0, 0, 255)\n",
        "        img[fs_line_resize==2, :] = (38, 255, 255)\n",
        "        \n",
        "        opac_image = (img/2 + copy_img/2).astype(np.uint8)\n",
        "        cv2.imwrite(os.path.join(predict_path, image.split(\"/\")[-1]), opac_image)\n",
        "        #print(\"mask size from model: \", mask.shape),\n",
        "        #print(\"resized mask size: \", mask_resize.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predict(fs_model, line_model, test_input_path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YeMCmDIM_ff"
      },
      "source": [
        "## **STEP 9: MULTI CLASS PREDICTION IN VIDEO DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0qbo6GsxTBY"
      },
      "source": [
        "!./darknet detector demo data/obj.data cfg/yolov4-obj.cfg /content/yolov4-obj_last.weights -dont_show /content/drive/MyDrive/Studies/Intern/test/videos/btest1.mp4 -thresh 0.4 -i 0 -out_filename /content/btest1_result.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zs5ulMMNP6I"
      },
      "source": [
        "### Video to Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9wBmZh_FYnf"
      },
      "source": [
        "!cp /content/drive/MyDrive/Studies/Intern/test/videos/test_wclass_sh.mp4 /content/test_wclass_sh.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLEskICmMXZs"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "# Opens the Video file\n",
        "if not os.path.exists(\"/content/data/\"): \n",
        "  os.mkdir(\"/content/data/\")\n",
        "if not os.path.exists(\"/content/data/test\"): \n",
        "  os.mkdir(\"/content/data/test\")\n",
        "for f in glob.glob(\"/content/data/test/*\"):\n",
        "  os.remove(f)\n",
        "\n",
        "cap = cv2.VideoCapture('/content/test_wclass_sh.mp4')\n",
        "i = 0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "    if ret == False:\n",
        "        break\n",
        "    cv2.imwrite('/content/data/test/'+str(i)+'.jpg',frame)\n",
        "    i+=1\n",
        " \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQfSBIXYOJYG"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ024z2vAVvx",
        "outputId": "c79ed62e-9361-4224-dc9c-81d4a96c1b84"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtZDPJm_pe7n"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import tqdm\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook\n",
        "from preprocess import tensorize_image\n",
        "import numpy as np\n",
        "from constant import *\n",
        "from train import *\n",
        "from natsort import natsorted\n",
        "\n",
        "#### PARAMETERS #####\n",
        "cuda = True\n",
        "test = True\n",
        "predict_name = \"test4_predicts\"\n",
        "fs_model_name = \"Unet_2.pt\"\n",
        "line_model_name = \"SegNet.pt\"\n",
        "fs_model_path = os.path.join(MODELS_DIR, fs_model_name)\n",
        "line_model_path = os.path.join(MODELS_DIR, line_model_name)\n",
        "input_shape = input_shape\n",
        "#####################\n",
        "\n",
        "if test:\n",
        "    if not os.path.exists(TEST_PREDICT_DIR): \n",
        "      os.mkdir(TEST_PREDICT_DIR)\n",
        "    test_input_path_list = glob.glob(os.path.join(TEST_DIR, \"*\"))\n",
        "    test_input_path_list = natsorted(test_input_path_list)\n",
        "    predict_path = os.path.join(TEST_PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "else:\n",
        "    if not os.path.exists(PREDICT_DIR): \n",
        "      os.mkdir(PREDICT_DIR)\n",
        "    predict_path = os.path.join(PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "\n",
        "if not os.path.exists(predict_path): \n",
        "    os.mkdir(predict_path)\n",
        "\n",
        "# LOAD MODEL\n",
        "fs_model = torch.load(fs_model_path)\n",
        "#Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. \n",
        "#Failing to do this will yield inconsistent inference results.\n",
        "fs_model.eval()\n",
        "\n",
        "line_model = torch.load(line_model_path)\n",
        "line_model.eval()\n",
        "\n",
        "if cuda:\n",
        "    fs_model = fs_model.cuda()\n",
        "    line_model = line_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# PREDICT\n",
        "def predict(fs_model, line_model, images):\n",
        "\n",
        "\n",
        "    for image in tqdm_notebook(images):\n",
        "        img = cv2.imread(image)\n",
        "        batch_test = tensorize_image([image], input_shape, cuda)\n",
        "        \n",
        "        fs_output = fs_model(batch_test)\n",
        "        line_output = line_model(batch_test)\n",
        "        fs_out = torch.argmax(fs_output, axis=1)\n",
        "        line_out = torch.argmax(line_output, axis=1)\n",
        "\n",
        "        \n",
        "        fs_out_cpu = fs_out.cpu()\n",
        "        line_out_cpu = line_out.cpu()\n",
        "        \n",
        "        fs_outputs_list  = fs_out_cpu.detach().numpy()\n",
        "        line_outputs_list  = line_out_cpu.detach().numpy()\n",
        "        \n",
        "        fs_mask = np.squeeze(fs_outputs_list, axis=0)\n",
        "        line_mask = np.squeeze(line_outputs_list, axis=0)\n",
        "       \n",
        "        fs_mask_uint8 = fs_mask.astype('uint8')\n",
        "        line_mask_uint8 = line_mask.astype('uint8')\n",
        "        \n",
        "        fs_mask_resize = cv2.resize(fs_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_CUBIC)\n",
        "        fs_line_resize = cv2.resize(line_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_NEAREST)\n",
        "        \n",
        "        \n",
        "        #img_resize = cv2.resize(img, input_shape)\n",
        "        mask_ind = fs_mask_resize == 1\n",
        "        mask_ind = fs_line_resize == 1\n",
        "        #copy_img = img_resize.copy()\n",
        "        copy_img = img.copy()\n",
        "        \n",
        "        img[fs_mask_resize==1, :] = (255, 0, 125)\n",
        "        img[fs_line_resize==1, :] = (0, 0, 255)\n",
        "        img[fs_line_resize==2, :] = (38, 255, 255)\n",
        "        \n",
        "        opac_image = (img/2 + copy_img/2).astype(np.uint8)\n",
        "        cv2.imwrite(os.path.join(predict_path, image.split(\"/\")[-1]), opac_image)\n",
        "        #print(\"mask size from model: \", mask.shape),\n",
        "        #print(\"resized mask size: \", mask_resize.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predict(fs_model, line_model, test_input_path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUaR8buSONmA"
      },
      "source": [
        "### Frame to Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_nZ2qN7wu5W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "95d01231e12f4c84be4f0c3543080a8e",
            "6f098a73f04f461db52f72e5def92264",
            "b9deb5e178c744ffaa6af2e10e79c390",
            "d1c43efb5abd48158cdd9739a61a9418",
            "809c9ed7612446b298d8aedad7bb5e66",
            "ffcc54a4974e458faa4e7e4bcc581e61",
            "7f418383366b4d3db5f04b62d7353add",
            "6ec54213372949d786b9bdb1f4e1e73a",
            "8be58601b6594ae5b12a77e8a61f1c47",
            "e58c342e8d7a408ea6a89d6dea60f4a3",
            "cc2db6f2674344ae86cb587ee3083e62",
            "367e8e3186f044b2a6062a980ccf4001",
            "0ca38c2126e3409d855117c2a9612072",
            "46b12b24e3b248bc8ffbdb427e1034cc",
            "11046eb8acc84e9c8fa7cf4a65556a95",
            "0dc1de87d0144fabaf89b2438f240b3d",
            "ccd1fb361a874c1d906ed8dbd5ce1f03",
            "0232ad05e7fa43f5af5e4372202432e0",
            "ba9d2ed703b549c7982a1c5bedf3d03a",
            "c20fc9165fab484ea263120afc3634ba",
            "4b8b64cc937e4f56a5dca1f03d9f49f1",
            "de4413398d854105bb4c119a968660f1"
          ]
        },
        "outputId": "3067dcd1-c3bd-4e82-e3fe-a0273feee3e8"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "pathIn= '/content/data/test_predicts/test_wclass_sh/'\n",
        "pathOut = '/content/test_wclass_sh.mp4'\n",
        "fps = 24\n",
        "frame_array = []\n",
        "files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
        "#for sorting the file names properly\n",
        "files = natsorted(files)\n",
        "for i in tqdm_notebook(range(len(files))):\n",
        "    filename=pathIn + files[i]\n",
        "    #reading each files\n",
        "    img = cv2.imread(filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    \n",
        "    #inserting the frames into an image array\n",
        "    frame_array.append(img)\n",
        "out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
        "for i in tqdm_notebook(range(len(frame_array))):\n",
        "    # writing to a image array\n",
        "    out.write(frame_array[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95d01231e12f4c84be4f0c3543080a8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/519 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367e8e3186f044b2a6062a980ccf4001",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/519 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbo8zKxUz5tR"
      },
      "source": [
        "!cp /content/test_wclass_sh.mp4 /content/drive/MyDrive/Studies/Intern/test/predicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWaSYOTHBMAj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG_SXrqOOTvL"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuIMK7C-O43c"
      },
      "source": [
        "## Step1: Crop Sign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bir4dy9qOVvi"
      },
      "source": [
        "import json\n",
        "import cv2\n",
        "\n",
        "def crop_sign():\n",
        "  global img\n",
        "  global positions\n",
        "  if not os.path.exists(\"/content/data/crop_image\"):\n",
        "    os.mkdir(\"/content/data/crop_image\")\n",
        "\n",
        "  for f in glob.glob(\"/content/data/crop_image/*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "  positions=[]\n",
        "\n",
        "  json_file = open('/content/darknet/result.json', 'r')#file reading process\n",
        "  json_dict=json.load(json_file)#Contents of json file converted to dict data type\n",
        "  \n",
        "  img= cv2.imread(json_dict[0][\"filename\"])\n",
        "\n",
        "  for i,obj in enumerate(json_dict[0][\"objects\"]):\n",
        "    if obj['class_id'] ==0:\n",
        "\n",
        "      center_x=img.shape[1]*obj[\"relative_coordinates\"]['center_x']\n",
        "      center_y=img.shape[0]*obj[\"relative_coordinates\"]['center_y']\n",
        "      width=img.shape[1]*obj[\"relative_coordinates\"]['width']\n",
        "      height=img.shape[0]*obj[\"relative_coordinates\"]['height']\n",
        "\n",
        "      #x_center=int(xmin + width()/2)\n",
        "      #y_center=int(ymin + height()/2)\n",
        "\n",
        "      new_width=(img.shape[1]*obj[\"relative_coordinates\"]['width'])/2\n",
        "      new_height=(img.shape[0]*obj[\"relative_coordinates\"]['height'])/2\n",
        "\n",
        "      x_min=abs(center_x-new_width)\n",
        "      y_min=abs(center_y-new_height)\n",
        "\n",
        "      #width=int(xmax-xmin)\n",
        "      #height=int(ymax-ymin)\n",
        "\n",
        "      x_max=width+x_min\n",
        "      y_max=height+y_min\n",
        "\n",
        "      w=int(width)\n",
        "      h=int(height)\n",
        "\n",
        "      positions.append([int(x_min), int(y_min), int(x_max),int(y_max)])\n",
        "      \n",
        "      crop_img = img[int(y_min):int(y_min)+h,int(x_min):int(x_min)+w]\n",
        "      ci_path=json_dict[0][\"filename\"].replace('test','crop_image')\n",
        "      cv2.imwrite(ci_path[:-4]+\"-\"+str(i)+\".jpg\",crop_img)\n",
        "  return positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odWmvi5tN6o4"
      },
      "source": [
        "positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSAaZcNNoQq"
      },
      "source": [
        "crop_sign()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oq3W2hIPhWx"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "for f in glob.glob(\"/content/full_predict/*\"):\n",
        "  os.remove(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMiDhaTaRU_b"
      },
      "source": [
        "## Step2: Class Pred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Thi1i5-PqH9"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras import models\n",
        "import glob\n",
        "\n",
        "def class_pred():\n",
        "  global pred\n",
        "  global meta_image\n",
        "  if not os.path.exists(\"/content/data/meta_image/\"):\n",
        "    os.mkdir(\"/content/data/meta_image/\")\n",
        "\n",
        "  for f in glob.glob(\"/content/data/meta_image/*\"):\n",
        "    os.remove(f)\n",
        "\n",
        "  crop_img=os.listdir('/content/data/crop_image')\n",
        "  \n",
        "  for f in crop_img:\n",
        "    if f.startswith(\".\"):\n",
        "      crop_img.remove(f)\n",
        "  \n",
        "  crop_img.sort()\n",
        "  data=[]\n",
        "  for i in crop_img:\n",
        "    image=cv2.imread(\"/content/data/crop_image/\"+i)\n",
        "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_fromarray = Image.fromarray(imageRGB, 'RGB')\n",
        "    resize_image = image_fromarray.resize((30, 30))\n",
        "    data.append(np.array(resize_image))\n",
        "\n",
        "  X_test = np.array(data)\n",
        "  X_test = X_test/255\n",
        "  model=models.load_model('/content/models/sign_ford_model.h5')\n",
        "  y = model.predict(X_test)\n",
        "  pred=np.argmax(y,axis=1)\n",
        "\n",
        "  for n,pr in enumerate(pred):\n",
        "    meta_path='/content/data/Meta/'\n",
        "    meta_image_path=meta_path+str(pr)+'.png'\n",
        "    meta_image=cv2.imread(meta_image_path, cv2.IMREAD_UNCHANGED)\n",
        "    cv2.imwrite('/content/data/meta_image/'+str(n)+'.jpg',meta_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VYvYhxQOzc"
      },
      "source": [
        "# **FINAL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAoSaxgTQ-KN",
        "outputId": "a7a6f950-6ae9-4aa9-85e3-d68a4134e1a6"
      },
      "source": [
        "%cd /content/darknet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/darknet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii3ZzOrBoTKM"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Studies/Intern/Meta /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPS-3r1vQRbT"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import tqdm\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook\n",
        "from preprocess import tensorize_image\n",
        "import numpy as np\n",
        "from constant import *\n",
        "from train import *\n",
        "from natsort import natsorted\n",
        "from PIL import Image\n",
        "\n",
        "#### PARAMETERS #####\n",
        "cuda = True\n",
        "test = True\n",
        "predict_name = \"test_pred_classification\"\n",
        "fs_model_name = \"Unet_2.pt\"\n",
        "line_model_name = \"SegNet.pt\"\n",
        "fs_model_path = os.path.join(MODELS_DIR, fs_model_name)\n",
        "line_model_path = os.path.join(MODELS_DIR, line_model_name)\n",
        "input_shape = input_shape\n",
        "#####################\n",
        "\n",
        "if test:\n",
        "    if not os.path.exists(TEST_PREDICT_DIR): \n",
        "      os.mkdir(TEST_PREDICT_DIR)\n",
        "\n",
        "\n",
        "    test_input_path_list = glob.glob(os.path.join(TEST_DIR, \"*\"))\n",
        "    test_input_path_list = natsorted(test_input_path_list)\n",
        "    predict_path = os.path.join(TEST_PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "    for f in glob.glob(predict_path+\"/*\"):\n",
        "      os.remove(f)\n",
        "else:\n",
        "    if not os.path.exists(PREDICT_DIR): \n",
        "      os.mkdir(PREDICT_DIR)\n",
        "    predict_path = os.path.join(PREDICT_DIR, predict_name.split(\".\")[0])\n",
        "\n",
        "if not os.path.exists(predict_path): \n",
        "    os.mkdir(predict_path)\n",
        "\n",
        "# LOAD MODEL\n",
        "fs_model = torch.load(fs_model_path)\n",
        "#Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. \n",
        "#Failing to do this will yield inconsistent inference results.\n",
        "fs_model.eval()\n",
        "\n",
        "line_model = torch.load(line_model_path)\n",
        "line_model.eval()\n",
        "\n",
        "if cuda:\n",
        "    fs_model = fs_model.cuda()\n",
        "    line_model = line_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# PREDICT\n",
        "def predict(fs_model, line_model, images):\n",
        "\n",
        "    global x_meta\n",
        "    for image in tqdm_notebook(images):\n",
        "        img = cv2.imread(image)\n",
        "        batch_test = tensorize_image([image], input_shape, cuda)\n",
        "\n",
        "        Detection(image)\n",
        "        img = cv2.imread(\"predictions.jpg\")\n",
        "\n",
        "        positions = crop_sign()\n",
        "\n",
        "        if not len(positions) == 0:\n",
        "          class_pred()\n",
        "        \n",
        "        fs_output = fs_model(batch_test)\n",
        "        line_output = line_model(batch_test)\n",
        "        fs_out = torch.argmax(fs_output, axis=1)\n",
        "        line_out = torch.argmax(line_output, axis=1)\n",
        "\n",
        "        \n",
        "        fs_out_cpu = fs_out.cpu()\n",
        "        line_out_cpu = line_out.cpu()\n",
        "        \n",
        "        fs_outputs_list  = fs_out_cpu.detach().numpy()\n",
        "        line_outputs_list  = line_out_cpu.detach().numpy()\n",
        "        \n",
        "        fs_mask = np.squeeze(fs_outputs_list, axis=0)\n",
        "        line_mask = np.squeeze(line_outputs_list, axis=0)\n",
        "       \n",
        "        fs_mask_uint8 = fs_mask.astype('uint8')\n",
        "        line_mask_uint8 = line_mask.astype('uint8')\n",
        "        \n",
        "        fs_mask_resize = cv2.resize(fs_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_CUBIC)\n",
        "        fs_line_resize = cv2.resize(line_mask_uint8, ((img.shape[1]), (img.shape[0])), interpolation = cv2.INTER_NEAREST)\n",
        "        \n",
        "        \n",
        "        #img_resize = cv2.resize(img, input_shape)\n",
        "        mask_ind = fs_mask_resize == 1\n",
        "        mask_ind = fs_line_resize == 1\n",
        "        #copy_img = img_resize.copy()\n",
        "        copy_img = img.copy()\n",
        "        \n",
        "        img[fs_mask_resize==1, :] = (255, 0, 125)\n",
        "        img[fs_line_resize==1, :] = (0, 0, 255)\n",
        "        img[fs_line_resize==2, :] = (38, 255, 255)\n",
        "        \n",
        "        opac_image = (img/2 + copy_img/2).astype(np.uint8)\n",
        "\n",
        "        if not len(positions) == 0:\n",
        "          for i, pos in enumerate(positions):\n",
        "            x_meta=cv2.imread('/content/data/meta_image/'+str(i)+'.jpg', cv2.IMREAD_UNCHANGED)\n",
        "            \n",
        "            x_len = int(pos[2]-pos[0])\n",
        "            y_len = int(pos[3]-pos[1])\n",
        "\n",
        "            \n",
        "            res_perc = int(min(x_len, y_len)*70/100)\n",
        "\n",
        "            if res_perc >= 40:\n",
        "              res_perc = 40\n",
        "\n",
        "            x_meta = cv2.resize(x_meta, (res_perc, res_perc))\n",
        "            \n",
        "            \n",
        "            #x = pos[2] - res_perc\n",
        "            #y = pos[3] - res_perc\n",
        "            x = pos[0]+2\n",
        "            y = pos[1]+2\n",
        "\n",
        "            '''\n",
        "            y1, y2 = y, y + x_meta.shape[0]\n",
        "            x1, x2 = x, x + x_meta.shape[1]\n",
        "\n",
        "            alpha_s = x_meta[:, :, 3] / 255.0\n",
        "            alpha_l = 1.0 - alpha_s\n",
        "\n",
        "            for c in range(0, 3):\n",
        "                opac_image[y1:y2, x1:x2, c] = (alpha_s * x_meta[:, :, c] +\n",
        "                                          alpha_l * opac_image[y1:y2, x1:x2, c]) \n",
        "            '''\n",
        "\n",
        "            opac_image[y:y+x_meta.shape[0],x:x+x_meta.shape[1]] = x_meta\n",
        "\n",
        "        cv2.imwrite(os.path.join(predict_path, image.split(\"/\")[-1]), opac_image)\n",
        "        #print(\"mask size from model: \", mask.shape),\n",
        "        #print(\"resized mask size: \", mask_resize.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    predict(fs_model, line_model, test_input_path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoqoU4A4rUqb",
        "outputId": "c4c68ca4-017b-4664-ee69-3ba415946896"
      },
      "source": [
        "x_meta.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEjni9eQtDRe"
      },
      "source": [
        "            x_len = int(pos[2]-pos[0])\n",
        "            y_len = int(pos[3]-pos[1])\n",
        "\n",
        "            if x_len<31 or y_len<31:\n",
        "              res = min(x_len, y_len)\n",
        "              x_meta = cv2.resize(x_meta, (res-5, res-5))\n",
        "            else:\n",
        "              x_meta = cv2.resize(x_meta, (30, 30))\n",
        "              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMsAod7tjU0j"
      },
      "source": [
        "### File Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0u4dKP6Vqhv"
      },
      "source": [
        "!cp /content/data/testx/569234_cfc_005061.jpg /content/data/test/\n",
        "#!cp /content/data/test_pred/4060_cfc_004060.jpg /content/data/test/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ycsBTiNfJhB"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/Studies/Intern/For_Colab /content/Intern\n",
        "!cp -r /content/drive/MyDrive/Studies/Intern/Meta /content/Intern\n",
        "!cp -r /content/drive/MyDrive/Studies/Intern/models /content/Intern\n",
        "!cp -r /content/drive/MyDrive/Studies/Intern/yolov4 /content/Intern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOat-JNthThY"
      },
      "source": [
        "!zip -r /content/test_pred_classification.zip /content/data/test_predicts/test_pred_classification/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ8E6kLef9-m"
      },
      "source": [
        "!cp /content/test_pred_classification.zip /content/drive/MyDrive/Studies/Intern/test/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efrfIhNJy3ma"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}